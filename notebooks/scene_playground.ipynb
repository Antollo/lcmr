{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from lcmr.grammar import Scene, Layer, Object, Appearance\n",
    "from lcmr.grammar.transformations import Affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scene(\n",
       "    layer=Layer(\n",
       "        composition=Tensor(shape=torch.Size([3, 2, 1]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
       "        object=Object(\n",
       "            appearance=Appearance(\n",
       "                color=Tensor(shape=torch.Size([3, 2, 4, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                confidence=Tensor(shape=torch.Size([3, 2, 4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                batch_size=torch.Size([3, 2, 4]),\n",
       "                device=None,\n",
       "                is_shared=False),\n",
       "            objectShape=Tensor(shape=torch.Size([3, 2, 4, 1]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
       "            transformation=Affine(\n",
       "                matrix=Tensor(shape=torch.Size([3, 2, 4, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                batch_size=torch.Size([3, 2, 4]),\n",
       "                device=None,\n",
       "                is_shared=False),\n",
       "            batch_size=torch.Size([3, 2, 4]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        scale=Tensor(shape=torch.Size([3, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        batch_size=torch.Size([3, 2]),\n",
       "        device=None,\n",
       "        is_shared=False),\n",
       "    batch_size=torch.Size([3]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene = Scene(\n",
    "    batch_size=[3],\n",
    "    layer=Layer(\n",
    "        batch_size=[3, 2],\n",
    "        object=Object(\n",
    "            batch_size=[3, 2, 4],\n",
    "            objectShape=torch.ones(3, 2, 4, 1, dtype=torch.uint8),\n",
    "            transformation=Affine(batch_size=[3, 2, 4], matrix=torch.ones(3, 2, 4, 3, 3)),\n",
    "            appearance=Appearance(batch_size=[3, 2, 4], confidence=torch.ones(3, 2, 4, 1), color=torch.ones(3, 2, 4, 3)),\n",
    "        ),\n",
    "        scale=torch.ones(3, 2, 1),\n",
    "        composition=torch.ones(3, 2, 1, dtype=torch.uint8),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kornia.geometry.transform import get_affine_matrix2d\n",
    "\n",
    "\n",
    "def create_scene(batch_len, layer_len, object_len, translation=None, scale=None, angle=None, color=None, confidence=None):\n",
    "    total_objects = batch_len * layer_len * object_len\n",
    "\n",
    "    if translation == None:\n",
    "        translation = torch.rand(total_objects, 2)\n",
    "    if scale == None:\n",
    "        scale = torch.rand(total_objects, 2)\n",
    "    if angle == None:\n",
    "        angle = torch.rand(total_objects, 1)\n",
    "    if color == None:\n",
    "        color = torch.rand(total_objects, 3)\n",
    "    color = color.reshape(batch_len, layer_len, object_len, 3)\n",
    "    if confidence == None:\n",
    "        confidence = torch.rand(total_objects, 1)\n",
    "    confidence = confidence.reshape(batch_len, layer_len, object_len, 1)\n",
    "\n",
    "    center = torch.zeros(total_objects, 2)\n",
    "    transformation = get_affine_matrix2d(translation, center, scale, angle.reshape(-1) * 360).reshape(batch_len, layer_len, object_len, 3, 3)\n",
    "\n",
    "    scene = Scene(\n",
    "        batch_size=[batch_len],\n",
    "        layer=Layer(\n",
    "            batch_size=[batch_len, layer_len],\n",
    "            object=Object(\n",
    "                batch_size=[batch_len, layer_len, object_len],\n",
    "                objectShape=torch.ones(batch_len, layer_len, object_len, 1, dtype=torch.uint8),\n",
    "                transformation=Affine(batch_size=[batch_len, layer_len, object_len], matrix=transformation),\n",
    "                appearance=Appearance(batch_size=[batch_len, layer_len, object_len], confidence=confidence, color=color),\n",
    "            ),\n",
    "            scale=torch.ones(batch_len, layer_len, 1),\n",
    "            composition=torch.ones(batch_len, layer_len, 1, dtype=torch.uint8),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def tensor_to_img(img: torch.Tensor):\n",
    "    return Image.fromarray((img.numpy() * 255).astype(np.uint8))\n",
    "\n",
    "\n",
    "def display_img(img):\n",
    "    # google colab uses white background for transparent images\n",
    "    # we overwrite that\n",
    "    display(\n",
    "        HTML(\n",
    "            \"\"\"\n",
    "            <style>\n",
    "            .output_image>img {\n",
    "                background-color: transparent !important;\n",
    "            }\n",
    "            </style>\n",
    "            \"\"\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    display(tensor_to_img(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lcmr.renderer import OpenGLRenderer2D\n",
    "\n",
    "renderer = OpenGLRenderer2D((200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = torch.tensor([[0, 0.], [0, 1], [1, 0]], dtype=torch.float32)\n",
    "color = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=torch.float32)\n",
    "scale = torch.tensor([[0.1, 0.5], [0.1, 0.1], [0.2, 0.2]])\n",
    "confidence = torch.tensor([[0.9], [0.8], [0.7]])\n",
    "angle = torch.tensor([[0], [0], [0]], dtype=torch.float32)\n",
    "\n",
    "scene = create_scene(1, 1, 3, translation=translation, scale=scale, color=color, confidence=confidence, angle=angle)\n",
    "img = renderer.render(scene)\n",
    "display_img(img[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
