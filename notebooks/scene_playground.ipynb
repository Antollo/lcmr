{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <style>\n",
       "            .cell-output-ipywidget-background {\n",
       "                background-color: transparent !important;\n",
       "            }\n",
       "            :root {\n",
       "                --jp-widgets-color: var(--vscode-editor-foreground);\n",
       "                --jp-widgets-font-size: var(--vscode-editor-font-size);\n",
       "            }  \n",
       "            </style>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from lcmr.grammar import Appearance, Layer, Object, Scene\n",
    "from lcmr.grammar.transformations import Affine, LazyAffine\n",
    "from lcmr.renderer.renderer2d import OpenGLRenderer2D\n",
    "from lcmr.utils.presentation import display_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Scene(\n",
       "    layer=Layer(\n",
       "        composition=Tensor(shape=torch.Size([3, 2, 1]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
       "        object=Object(\n",
       "            appearance=Appearance(\n",
       "                color=Tensor(shape=torch.Size([3, 2, 4, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                confidence=Tensor(shape=torch.Size([3, 2, 4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                batch_size=torch.Size([3, 2, 4]),\n",
       "                device=None,\n",
       "                is_shared=False),\n",
       "            objectShape=Tensor(shape=torch.Size([3, 2, 4, 1]), device=cpu, dtype=torch.uint8, is_shared=False),\n",
       "            transformation=LazyAffine(\n",
       "                angle=Tensor(shape=torch.Size([3, 2, 4, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                scale=Tensor(shape=torch.Size([3, 2, 4, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                translation=Tensor(shape=torch.Size([3, 2, 4, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                rotation_vec=None,\n",
       "                batch_size=torch.Size([3, 2, 4]),\n",
       "                device=None,\n",
       "                is_shared=False),\n",
       "            efd=None,\n",
       "            fd=None,\n",
       "            shapeLatent=None,\n",
       "            contour=None,\n",
       "            _n_points=-1,\n",
       "            batch_size=torch.Size([3, 2, 4]),\n",
       "            device=None,\n",
       "            is_shared=False),\n",
       "        scale=Tensor(shape=torch.Size([3, 2, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        batch_size=torch.Size([3, 2]),\n",
       "        device=None,\n",
       "        is_shared=False),\n",
       "    backgroundColor=None,\n",
       "    batch_size=torch.Size([3]),\n",
       "    device=None,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene = Scene(\n",
    "    batch_size=[3],\n",
    "    layer=Layer(\n",
    "        batch_size=[3, 2],\n",
    "        object=Object(\n",
    "            batch_size=[3, 2, 4],\n",
    "            objectShape=torch.ones(3, 2, 4, 1, dtype=torch.uint8),\n",
    "            #transformation=Affine(batch_size=[3, 2, 4], matrix=torch.ones(3, 2, 4, 3, 3)),\n",
    "            transformation=LazyAffine.from_tensors(translation=torch.ones(3, 2, 4, 2), scale=torch.ones(3, 2, 4, 2), angle=torch.ones(3, 2, 4, 1)),\n",
    "            appearance=Appearance(batch_size=[3, 2, 4], confidence=torch.ones(3, 2, 4, 1), color=torch.ones(3, 2, 4, 3)),\n",
    "        ),\n",
    "        scale=torch.ones(3, 2, 1),\n",
    "        composition=torch.ones(3, 2, 1, dtype=torch.uint8),\n",
    "    ),\n",
    ")\n",
    "\n",
    "\n",
    "scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kornia.geometry.transform import get_affine_matrix2d\n",
    "\n",
    "\n",
    "def create_scene(batch_len: int, layer_len: int, object_len: int, translation=None, scale=None, angle=None, color=None, confidence=None) -> Scene:\n",
    "    total_objects = batch_len * layer_len * object_len\n",
    "\n",
    "    if translation == None:\n",
    "        translation = torch.rand(total_objects, 2)\n",
    "    if scale == None:\n",
    "        scale = torch.rand(total_objects, 2)\n",
    "    if angle == None:\n",
    "        angle = torch.rand(total_objects, 1)\n",
    "    if color == None:\n",
    "        color = torch.rand(total_objects, 3)\n",
    "    color = color.reshape(batch_len, layer_len, object_len, 3)\n",
    "    if confidence == None:\n",
    "        confidence = torch.rand(total_objects, 1)\n",
    "    confidence = confidence.reshape(batch_len, layer_len, object_len, 1)\n",
    "\n",
    "    center = torch.zeros(total_objects, 2)\n",
    "    transformation = get_affine_matrix2d(translation, center, scale, angle.reshape(-1) * 360).reshape(batch_len, layer_len, object_len, 3, 3)\n",
    "\n",
    "    scene = Scene(\n",
    "        batch_size=[batch_len],\n",
    "        layer=Layer(\n",
    "            batch_size=[batch_len, layer_len],\n",
    "            object=Object(\n",
    "                batch_size=[batch_len, layer_len, object_len],\n",
    "                objectShape=torch.ones(batch_len, layer_len, object_len, 1, dtype=torch.uint8),\n",
    "                transformation=Affine(batch_size=[batch_len, layer_len, object_len], matrix=transformation),\n",
    "                appearance=Appearance(batch_size=[batch_len, layer_len, object_len], confidence=confidence, color=color),\n",
    "            ),\n",
    "            scale=torch.ones(batch_len, layer_len, 1),\n",
    "            composition=torch.ones(batch_len, layer_len, 1, dtype=torch.uint8),\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "renderer = OpenGLRenderer2D((200, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMgAAADICAYAAACtWK6eAAAIT0lEQVR4Ae3dv5bcNBiG8U1OOqih4RZoFhq4EO4g0HM3hAuiARp6GmhITw3+DqtgNsu788djS/ZP5+yRJ9+MJT/6nkiyJ5sXP9/d/XW3Ynk9tfXTiu1pqncCn3bdwZdr9+77qcHP1m5UewhcSGB1QaqfJYmCwAgENhGkwLwZgY4+rkDgzxXauLyJzQT5fOozSS4fOJ9ch8BmgtTllSTfrHOdWkHgIgKbClI9LkFIctHY7eRDf3R9HZsLUnRI0nWOHLpzXQhCkiPnoE36yaNfM4lnJCfj2tEb+5WkmxmkjbYHiY2EugcC3QlSUGzae0iNNfvQ70a9S0E8I1kzOXtoyxLr7FHwjORsZIN/oE9JupxB2ki7/dtIHKHuc5nVtSCVFiQ5ghx1jTWD9DeLdC9IoXP7tygcofQ3iwwhSKWG279HEMQMctUou/17Fb5BPvxrV/0cZgYpau5sdZU7N+pMX3uRoQSpEbFpv1FednXafvYiwwnSJPGdra4yeuHO9DOLDClIjYZN+8I52d3p+tiLDCtIjadNe3dZvXCHtpdkaEFs2hfOx+5OV0utbfcjQwtS42nT3l1WL9yht9P5tns+MrwgNRoliU17kdhrqaXWNpLsQpBKC7+Mbq9ytOvaZqn18nVrfwe137O1g0H830uoGeSX6WfdmeRl/SLpvUhi0/6/2bWjwLrLrRcTuXe/3b3+Bq4kG72U8H6D/Oij+Fz/P5re8PFzb7o6/h9B6mx7keT+ajRO0D+B20vy3ib964nKd/2TebaH9iPPItrBG95O13Dbzft7ghS1EmR0SexHaiSPUEqS2rzfRpT3llhzpHt4CGc/Mh/RvR8vv+R6cgZpGPcwk5TkylEILD+bREEK6+iSWGodRY75dS4nSlxizZusr3KM/LTaUms+mkc7/mC64Po5/7bwyYI0pG+mgxGflfw49bvu0ClHJ9Bk+XACUce5nC1InW7UzTtJcjIcM9qEqasvaar8K85FgtQpRpXEUqtGTzmVwMWCVAOjSnJ/Kh3vOzyBZ+9iJUJ1h6v+Rh6tlNgKAqcQuEqQaqC+FFh/I9f6fpRSgvgHVqOM1rb9vFqQ1v3RvsNlFmkjp04EFhOkGhnpoaIHiCktxBqBqzbp7SSP65E27+5qPR49r+cEFp1B2onb5n2EfYmlVhs19VMEbiJINVSb9xH2JbXUsmGvEVOeInAzQVpjI+xLzCJttNSPCdxckGqwd0nMIo/TwutGYBVBqrGS5P6hrte9FbNIbyPSR39WE6Rdbq+zidu+bYTUcwKrC1KN9yqJzfo8NRwXgU0EqYZ7XHLZi9TIKHMCmwnSOtHbbGIv0kZGXQQ2F6Q6UZLUE+0eHiyaRWpElEagC0GqMz09WDSLtPRQdyNIG4oellxmkTYa6u4EqSHpYQNvFiFHEbjJt3mXRFuJulWy3i95Ic41JIEuZ5A5yS1nE89F5iNxzOPuBWnD0sPepPVFfRwCwwhSQ7L2bLLV0u446df/lXa/B0kI19if2IekEdh/bKgZ5PFwWHY9JuL10gSGFqRgrL3sWnoAnK9vAsML0vASpZFQL0lgN4I0KEuK0sN3w9p1qbchsDtBGsa5KHV8SfFfSV9CbV+fGfou1jlDUQ/9zvlXgyXVpWKd0y/v7ZvAYQSZD0OTpdXzWC2rSgyzx5zKcY8PKchxh9uVn0tgt3uQc0F4PwJPESDIU1T8GQIPBAgiFRAIBAgS4AghQBA5gEAgQJAARwgBgsgBBAIBggQ4QggQRA4gEAgQJMARQoAgcgCBQIAgAY4QAgSRAwgEAgQJcIQQIIgcQCAQIEiAI4QAQeQAAoEAQQIcIQQIIgcQCAQIEuAIIUAQOYBAIECQAEcIAYLIAQQCAYIEOEIIEEQOIBAIECTAEUKAIHIAgUCAIAGOEAIEkQMIBAIECXCEECCIHEAgECBIgCOEAEHkAAKBAEECHCEECCIHEAgECBLgCCFAEDmAQCBAkABHCAGCyAEEAgGCBDhCCBBEDiAQCBAkwBFCgCByAIFAgCABjhACBJEDCAQCBAlwhBAgiBxAIBAgSIAjhABB5AACgQBBAhwhBAgiBxAIBAgS4AghQBA5gEAgQJAARwgBgsgBBAIBggQ4QggQRA4gEAgQJMARQoAgcgCBQIAgAY4QAgSRAwgEAgQJcIQQIIgcQCAQIEiAI4QAQeQAAoEAQQIcIQQIIgcQCAQIEuAIIUAQOYBAIECQAEcIAYLIAQQCAYIEOEIIEEQOIBAIECTAEUKAIHIAgUCAIAGOEAIEkQMIBAIECXCEECCIHEAgECBIgCOEAEHkAAKBAEECHCEECCIHEAgECBLgCCFAEDmAQCBAkABHCAGCyAEEAgGCBDhCCBBEDiAQCBAkwBFCgCByAIFAgCABjhACBJEDCAQCBAlwhBAgiBxAIBAgSIAjhABB5AACgQBBAhwhBAgiBxAIBAgS4AghQBA5gEAgQJAARwgBgsgBBAIBggQ4QggQRA4gEAgQJMARQoAgcgCBQODV3bcP0d+n+reH46rrtYLAwQm8enf9n0xH9VPli3+qux+mun4UBA5KIC+xSpSaYZowB4Xkso9LIAvSuBClkVAfjMBpgjQoJYrZpNFQH4DAeYIUEJIcIC1cYiNwviD1SZI0fuqdE7hMkIJCkp2nhssrApcLUp8uSdqt4XqtILAzAtcJUjC+2hkRl4PAjMD1gtTJaiZRENghgeUEsdTaYXq4pGUEKY5fgonA/ggsJ8j+2LgiBK68izUHOP+y4/zPHSMwMAEzyMCDp+u3J7CsIPYhtx8xLaxKYFlBVu26xhC4PYFlBXGr9/YjpoVVCSwryKpd1xgCtyfwN9yspd5rsaO6AAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.Image.Image image mode=RGBA size=200x200>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "translation = torch.tensor([[0, 0.], [0, 1], [1, 0]], dtype=torch.float32)[None, None, ...]\n",
    "color = torch.tensor([[0.9, 0, 0], [0, 0.9, 0], [0, 0, 0.9]], dtype=torch.float32)[None, None, ...]\n",
    "scale = torch.tensor([[0.1, 0.5], [0.1, 0.1], [0.2, 0.2]])[None, None, ...]\n",
    "confidence = torch.tensor([[0.9], [0.5], [0.2]])[None, None, ...]\n",
    "angle = torch.tensor([[-np.pi/4], [0], [0]], dtype=torch.float32)[None, None, ...]\n",
    "scene = Scene.from_tensors_sparse(translation=translation, scale=scale, color=color, confidence=confidence, angle=angle)\n",
    "\n",
    "display_img(renderer.render(scene).image_rgb[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
